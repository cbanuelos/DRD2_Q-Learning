{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">Q-Agent on IGT      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game plan\n",
    "\n",
    "1. add loss (negative rewards) possibility rather then have choices have either 1 positive posibility or 0\n",
    " - new variable 'pvalues' in objects: MultiArmedBandit, Qagent\n",
    "\n",
    "\n",
    "\n",
    "2. add a 2nd alpha for loss (negative rewards)\n",
    " - changes need to be made in: MultiArmedBandit object, Qagent object\n",
    " - include conditional that checks for + or - negative \"rewards\" and changes alpha before computing (r is (+)rewards, p is (-)rewards):\n",
    " \t\t\t\tQ(s,a) += alpha_g * (r + max,Q(s') - Q(s,a)) \n",
    " \t\t\t\tQ(s,a) += alpha_l * (p + max,Q(s') - Q(s,a)) \n",
    "\n",
    "\n",
    "3. use the actual decks of cards used for the IGT instead of randomized values for the decks\n",
    " - deck will come from a csv type file\n",
    "\n",
    "\n",
    "4. explore the 3D of the parameter space - alpha, beta pairs and payoff\n",
    " - different alpha ratios\n",
    " - different alpha amplitudes\n",
    " - different beta amplitudes\n",
    " \n",
    " \n",
    " \n",
    "5. change alpha used depending on RPE (reward prediction error), in update_Qi(), or whether (reward - Qval) >= 0, reward gains or (reward - Qval) < 0, reward loss\n",
    "\n",
    "\n",
    "6. change all code from \"Alpha reward\" to \"Alpha gains\", and \"Alpha punishments\" to \"Alpha loss\"\n",
    "\n",
    "7. Find the distribution (mean, histogram) of Payoff & Sensitivity scores from behavioral data\n",
    "\n",
    "8. create a 2D heat maps for assymetry of learning and explore/exploit values\n",
    " - pick out agents that are greedy, moderate, etc, etc\n",
    " \n",
    "       - Phase 1: test out all alpha gains in (positive range) and inverse for alpha losses, and 4 different betas (positive range). \n",
    "       Goal: find alpha amplitudes and betas that produce similar mean and variance as behavioral data.\n",
    "     \n",
    "       - Phase 2: keep alpha gains constant, and vary alpha loss (decending order, and then ascending order from inverse). \n",
    "       Goal: to find optimal alpha ratios and betas that produce similar mean and variance as behavioral data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Thoughts\n",
    "- create a curve of explore-exploit trade-off where the greediness increase the longer you've play and perhaps plataus as you've \n",
    "\n",
    "- should learning rate be 0 for losses, or negative RPE's?\n",
    " \n",
    "---\n",
    "\n",
    "## IGT game design \n",
    "100 Trials total (from Bechara, 1997)\n",
    "\n",
    "|  | Deck A | Deck B | Deck C | Deck D |\n",
    "| -------- | --- | --- | --- | --- | --- |\n",
    "| p(gains) | 0.5 | 0.9 | 0.5 | 0.9 |\n",
    "| g(losses) | 0.5 | 0.1 | 0.5 | 0.1 |\n",
    "| avg gains | \\$100 | \\$100 | \\$50 | \\$50 |\n",
    "| avg losses | -\\$250 | -\\$1250 | -\\$50 | -\\$250 |\n",
    "| overall gains | -\\$75 | -\\$75 | \\$0 | \\$20 |\n",
    "\n",
    "\n",
    "#### Alternative designs\n",
    "- switch overall gains of C & D so that C has positive gains and keeps p(gains)=0.5, \n",
    "\tand D has $0 gains and keeps p(gains)=0.9\n",
    "\t- may be interesting to see the effect of alpha on frequency of gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy.random import sample as rs\n",
    "from numpy import newaxis as na\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from scipy.stats import sem\n",
    "import seaborn as sns\n",
    "import string\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import csv\n",
    "from __future__ import division\n",
    "from future.utils import listvalues\n",
    "from scipy.stats.stats import sem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Qi(Qval, reward, alpha):\n",
    "    \"\"\" update q-value of selected action, given reward and alpha\n",
    "    alpha is changed depending on RPE (reward prediction error)\n",
    "    (whether (reward - Qval) >= 0, reward gains or (reward - Qval) < 0, reward loss)\n",
    "    \"\"\"\n",
    "    \n",
    "#     if (reward - Qval) >= 0:\n",
    "#         alpha = alpha_g\n",
    "#     if (reward - Qval) < 0:\n",
    "#         alpha = alpha_l\n",
    "    \n",
    "    return Qval + alpha * (reward - Qval)\n",
    "\n",
    "\n",
    "def update_Pall(Qvector, beta):\n",
    "    \"\"\" update vector of action selection probabilities given\n",
    "    associated q-values\n",
    "    \"\"\" \n",
    "    Zvector = Qvector - max(Qvector)\n",
    "    denom = np.sum(np.exp(beta * Zvector))\n",
    "        \n",
    "    resulting_pdata = np.array([np.exp(beta*Q_i) / denom for Q_i in Zvector])\n",
    "    \n",
    "    return resulting_pdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IowaGamblingTask(object):\n",
    "    \"\"\" defines a multi-armed bandit task\n",
    "\n",
    "    ::Arguments::\n",
    "        preward (list): 1xN vector of reward probabilities for each of N bandits\n",
    "        rvalues (list): 1xN vector of payout values for each of N bandits\n",
    "    \"\"\"    \n",
    "            \n",
    "    def __init__(self):\n",
    "                \n",
    "        self.all_cards = pd.read_csv('IGTCards.csv')\n",
    "        self.deck_gains = self.all_cards.sum()\n",
    "        self.deck_counters = np.zeros(len(self.all_cards.columns), dtype = int)\n",
    "        \n",
    "\n",
    "\n",
    "    def get_feedback(self, action_ix):\n",
    "        \n",
    "        \n",
    "        if self.deck_counters[action_ix] == 49:\n",
    "            self.deck_counters[action_ix] = 0\n",
    "        \n",
    "        else:    \n",
    "            self.deck_counters[action_ix] += 1\n",
    "        \n",
    "        curr_counter = self.deck_counters[action_ix]\n",
    "        \n",
    "        feedback = self.all_cards.iloc[curr_counter, action_ix]\n",
    "        \n",
    "        return feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qagent(object):\n",
    "\n",
    "    \"\"\" defines the learning parameters of single q-learning agent\n",
    "    in a multi-armed bandit task\n",
    "\n",
    "    ::Arguments::\n",
    "        alpha_g (float): learning rate for gains\n",
    "        alpha_l (float): learning rate for losses\n",
    "        beta (float): inverse temperature parameter\n",
    "        preward (list): 1xN vector of reward probaiblities for each of N decks\n",
    "        rvalues (list): 1xN vector of payout values for each of N decks\n",
    "                        IF rvalues is None, all values set to 1\n",
    "        pvalues (list): 1xN vector of punishment values for each of N decks\n",
    "                        IF rvalues is None, all values set to 1\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha_g,\n",
    "                       alpha_l, \n",
    "                       beta, \n",
    "                       epsilon=.1, \n",
    "                       decks=['A', 'B', 'C', 'D']):\n",
    "\n",
    "        if decks is None:\n",
    "            decks = ['A', 'B', 'C', 'D']\n",
    "\n",
    "        # calling IowaGamblingTask() function with arguments in Qagent() object\n",
    "        self.IGT = IowaGamblingTask()\n",
    "        \n",
    "        self.alpha_data = []\n",
    "        \n",
    "        self.rpe_data = []\n",
    "        \n",
    "        #initializing alpha(r) function to set which alpha to use depending on +-r\n",
    "        #self.alpha = lambda r: alpha_g if r > 0 else alpha_l\n",
    "        \n",
    "        # setting parameters passed through Qagent() as arguments\n",
    "        self.set_params(alpha_g=alpha_g, alpha_l=alpha_l, beta=beta, epsilon=epsilon, decks=decks)\n",
    "\n",
    "\n",
    "    def set_params(self, **kwargs):\n",
    "        \n",
    "        \"\"\" update parameters of q-learning agent:\n",
    "                alpha_g = learning rate for gains\n",
    "                alpha_l = learning rate for losses\n",
    "                beta = inv. temperature,\n",
    "                epsilon = exploration constant to randomize decisions\n",
    "                preward = probability of reward, p(reward)\n",
    "                rvalues = reward amounts  (+$)\n",
    "                pvalues = punishment amounts (-$)\n",
    "        \"\"\"\n",
    "\n",
    "        kw_keys = list(kwargs)\n",
    "\n",
    "        if 'alpha_g' in kw_keys:\n",
    "            self.alpha_g = kwargs['alpha_g']\n",
    "\n",
    "        if 'alpha_l' in kw_keys:\n",
    "            self.alpha_l = kwargs['alpha_l']\n",
    "\n",
    "        if 'beta' in kw_keys:\n",
    "            self.beta = kwargs['beta']\n",
    "\n",
    "        if 'epsilon' in kw_keys:\n",
    "            self.epsilon = kwargs['epsilon']\n",
    "        \n",
    "        if 'decks' in kw_keys:\n",
    "            self.decks = kwargs['decks']\n",
    "\n",
    "        # number of choices/options\n",
    "        self.nact = len(self.decks)\n",
    "\n",
    "        # actions limited to number of choices/options\n",
    "        self.actions = np.arange(self.nact)\n",
    "\n",
    "\n",
    "    def play_IGT(self, ntrials=100, get_output=True):\n",
    "        \n",
    "        \"\"\" simulates agent performance on a multi-armed bandit task\n",
    "\n",
    "        ::Arguments::\n",
    "            ntrials (int): number of trials to play bandits\n",
    "            get_output (bool): returns output DF if True (default)\n",
    "\n",
    "        ::Returns::\n",
    "            DataFrame (Ntrials x Nbandits) with trialwise Q and P\n",
    "            values for each bandit\n",
    "        \"\"\"\n",
    "        \n",
    "        pdata = np.zeros((ntrials + 1, self.nact))\n",
    "        \n",
    "        pdata[0, :] = np.array([1/self.nact]*self.nact)\n",
    "        \n",
    "        qdata = np.zeros_like(pdata)\n",
    "        self.choices = []\n",
    "        self.feedback = []\n",
    "\n",
    "        for t in range(ntrials):\n",
    "\n",
    "            # select bandit arm (action)            \n",
    "            act_i = np.random.choice(self.actions, p=pdata[t, :])\n",
    "            \n",
    "            # observe feedback\n",
    "            r = self.IGT.get_feedback(act_i)\n",
    "\n",
    "            # update value of selected action depending on whether it is a gain or loss\n",
    "            rpe = r - qdata[t, act_i]\n",
    "            if rpe >= 0:\n",
    "                alpha = self.alpha_g\n",
    "            if rpe < 0:\n",
    "                alpha = self.alpha_l\n",
    "            \n",
    "            qdata[t+1, act_i] = update_Qi(qdata[t, act_i], r, alpha)\n",
    "\n",
    "            # broadcast old q-values for unchosen actions\n",
    "            for act_j in self.actions[np.where(self.actions!=act_i)]:\n",
    "                qdata[t+1, act_j] = qdata[t, act_j]\n",
    "\n",
    "            # update action selection probabilities and store data\n",
    "            pdata[t+1, :] = update_Pall(qdata[t+1, :], self.beta)\n",
    "            \n",
    "            self.choices.append(act_i)\n",
    "            self.feedback.append(r)\n",
    "            self.rpe_data.append(rpe)\n",
    "            self.alpha_data.append(alpha)\n",
    "        \n",
    "        self.pdata = pdata[1:, :]\n",
    "        self.qdata = qdata[1:, :]\n",
    "        self.make_output_df()\n",
    "\n",
    "        if get_output:\n",
    "            return self.data.copy()\n",
    "\n",
    "\n",
    "    def make_output_df(self):\n",
    "        \"\"\" generate output dataframe with trialwise Q and P measures for each bandit,\n",
    "        as well as choice selection, and feedback\n",
    "        \"\"\"\n",
    "        df = pd.concat([pd.DataFrame(dat) for dat in [self.qdata, self.pdata]], axis=1)\n",
    "        columns = np.hstack(([['{}{}'.format(x, c) for c in self.actions] for x in ['q', 'p']]))\n",
    "        df.columns = columns\n",
    "        df.insert(0, 'trial', np.arange(1, df.shape[0]+1))\n",
    "        df['choice'] = self.choices\n",
    "        df['feedback'] = self.feedback\n",
    "        \n",
    "        # replace 3 with self.IGT.deck_gains.values.argmax()\n",
    "        df['optimal'] = np.where(df['choice']==3, 1, 0)\n",
    "        df['RPE'] = self.rpe_data\n",
    "        df['alpha'] = self.alpha_data\n",
    "        df.insert(0, 'agent', 1)\n",
    "        self.data = df.copy()\n",
    "\n",
    "\n",
    "    def simulate_multiple(self, nsims=10, ntrials=1000):\n",
    "        \"\"\" simulates multiple identical agents on multi-armed bandit task\n",
    "        \"\"\"\n",
    "        dflist = []\n",
    "        for i in range(nsims):\n",
    "            data_i = self.play_bandits(ntrials=ntrials, get_output=True)\n",
    "            data_i['agent'] += i\n",
    "            dflist.append(data_i)\n",
    "        return pd.concat(dflist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_auc(df, nblocks=25, verbose=False, as_percent=True):\n",
    "    xdf = blockify_trials(df, nblocks=nblocks)\n",
    "    muOptDF = xdf.groupby(['agent', 'block']).mean().reset_index()\n",
    "    auc = pd.pivot_table(muOptDF, values='optimal', index='block').values.sum()\n",
    "    if as_percent:\n",
    "        auc = (auc / nblocks) * 100\n",
    "        print(\"Optimal Choice chosen {:.2f}% of time\".format(auc))\n",
    "    if verbose:\n",
    "        print(\"Optimal Choice chosen {:.2f} times\".format(auc))\n",
    "\n",
    "    return auc\n",
    "\n",
    "def analyze_bandits(df, nblocks=25, get_err=False):\n",
    "    xdf = blockify_trials(df, nblocks=nblocks)\n",
    "    optDF = xdf.groupby(['agent', 'block']).mean().reset_index()\n",
    "    muOpt = pd.pivot_table(optDF, values='optimal', index='block').values\n",
    "    muOpt = np.hstack(muOpt)\n",
    "    if get_err:\n",
    "        errOpt = pd.pivot_table(optDF, values='optimal', index='block', aggfunc=sem).values*1.96\n",
    "        errOpt = np.hstack(errOpt)\n",
    "    else:\n",
    "        errOpt = np.zeros_like(muOpt)\n",
    "    return muOpt, errOpt\n",
    "\n",
    "def blockify_trials(data, nblocks=5, conds=None, groups=['agent']):\n",
    "\n",
    "    datadf = data.copy()\n",
    "    if conds is not None:\n",
    "        if type(conds) is str:\n",
    "            conds = [conds]\n",
    "        groups = groups + conds\n",
    "\n",
    "    idxdflist = []\n",
    "    for dfinfo, idxdf in datadf.groupby(groups):\n",
    "        ixblocks = np.array_split(idxdf.trial.values, nblocks)\n",
    "        blocks = np.hstack([[i+1]*arr.size for i, arr in enumerate(ixblocks)])\n",
    "        idxdf = idxdf.copy()\n",
    "        colname = 'block'\n",
    "        idxdf[colname] = blocks\n",
    "        idxdflist.append(idxdf)\n",
    "\n",
    "    return pd.concat(idxdflist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_qlearning(data, nblocks=25, analyze=True):\n",
    "\n",
    "    if analyze:\n",
    "        auc = get_optimal_auc(data, nblocks, as_percent=True)\n",
    "\n",
    "    sns.set(style='white', font_scale=1.3)\n",
    "    clrs = ['#3778bf', '#feb308', '#9b59b6', '#2ecc71', '#e74c3c',\n",
    "            '#3498db', '#fd7f23', '#694098', '#319455', '#f266db',\n",
    "            '#13579d', '#fa8d67'  '#a38ff1'  '#3caca4', '#c24f54']\n",
    "\n",
    "    f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12,3.5))\n",
    "    df = data.copy()\n",
    "    nactions = int(df.columns[-4].split('p')[-1])+1\n",
    "    actions = np.arange(nactions)\n",
    "\n",
    "    mudf = df.groupby('trial').mean().reset_index()\n",
    "    errdf = df.groupby('trial').sem().reset_index()*1.96\n",
    "    x = mudf.trial.values\n",
    "\n",
    "    plot_err = True\n",
    "    if np.isnan(errdf.loc[1, 'q0']):\n",
    "        plot_err = False\n",
    "\n",
    "    x3 = np.arange(1, nblocks+1)\n",
    "    chance = 1/nactions\n",
    "    mu3, err3 = analyze_bandits(df, nblocks=nblocks, get_err=plot_err)\n",
    "    ax3.plot(x3, mu3, color='k')\n",
    "    ax3.hlines(chance, 1, x3[-1], color='k', linestyles='--', label='chance')\n",
    "\n",
    "    for i, act in enumerate(actions):\n",
    "        muQ = mudf['q{}'.format(act)].values\n",
    "        muP = mudf['p{}'.format(act)].values\n",
    "        ax1.plot(x, muQ, label='$arm_{}$'.format(i), color=clrs[i])\n",
    "        ax2.plot(x, muP, color=clrs[i])\n",
    "\n",
    "        if plot_err:\n",
    "            errQ = errdf['q{}'.format(act)].values\n",
    "            errP = errdf['p{}'.format(act)].values\n",
    "            ax1.fill_between(x, muQ-errQ, muQ+errQ, color=clrs[i], alpha=.2)\n",
    "            ax2.fill_between(x, muP-errP, muP+errP, color=clrs[i], alpha=.2)\n",
    "            if i==0:\n",
    "                ax3.fill_between(x3, mu3-err3, mu3+err3, color='k', alpha=.15)\n",
    "        else:\n",
    "            ychance = np.ones(mu3.size) * chance\n",
    "            mu3A = np.copy(mu3)\n",
    "            mu3B = np.copy(mu3)\n",
    "            mu3A[np.where(mu3<=chance)] = chance\n",
    "            mu3B[np.where(mu3>=chance)] = chance\n",
    "            ax3.fill_between(x3, ychance, mu3A, color='#2ecc71', alpha=.15)\n",
    "            ax3.fill_between(x3, ychance, mu3B, color='#e74c3c', alpha=.15)\n",
    "\n",
    "    ax1.legend(loc=4)\n",
    "    ax1.set_ylabel('$Q(arm)$')\n",
    "    ax1.set_title('Value')\n",
    "\n",
    "    ax2.set_ylabel('$P(arm)$')\n",
    "    ax2.set_ylim(0,1)\n",
    "    ax2.set_title('Softmax Prob.')\n",
    "\n",
    "    ax3.set_ylim(0,1)\n",
    "    ax3.set_ylabel('% Optimal Deck')\n",
    "    ax3.set_xticks([1, nblocks+1])\n",
    "    ax3.set_xticklabels([1, df.trial.max()])\n",
    "    ax3.legend(loc=4)\n",
    "\n",
    "    for ax in f.axes:\n",
    "        ax.set_xlabel('Trials')\n",
    "    plt.tight_layout()\n",
    "    sns.despine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IGT_scores(data):\n",
    "    \n",
    "    # initializing a choice dictionary with the default of 0 times chosen\n",
    "    choice_dict = {0: 0, 1:0, 2:0, 3:0}\n",
    "    \n",
    "    # updating the choice dictionary with the deck choices made\n",
    "    choices_made = data['choice'].value_counts(sort = False).to_dict()\n",
    "    \n",
    "    for key, value in choices_made.items():\n",
    "        choice_dict[key] = value\n",
    "\n",
    "    A, B, C, D = choice_dict.get(0), choice_dict.get(1), choice_dict.get(2), choice_dict.get(3)\n",
    "    \n",
    "    # Payoff (P)\n",
    "    payoff = (C + D) - (A + B)\n",
    "    \n",
    "    # Sensitivity to frequency of gains (Q)\n",
    "    sensitivity = (B + D) - (A + C)\n",
    "    \n",
    "    return pd.Series((payoff, sensitivity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmaps(data):\n",
    "    \n",
    "    plt.figure(figsize=(16, 16))\n",
    "    heatmap_df = pd.pivot(data, index = \"Alpha Loss\", columns = \"Alpha Gain\", values = \"Payoff\").astype('float')\n",
    "    \n",
    "    \n",
    "    sns.heatmap(heatmap_df, linewidths=.2, cmap='RdBu_r', vmin=-100, vmax=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_df(amin, amax, astep, given_beta):\n",
    "    df_columns = np.array(['Alpha Gain', 'Alpha Loss', 'Beta', 'Payoff', 'Sensitivity'])\n",
    "    df = pd.DataFrame(columns=df_columns)\n",
    "    \n",
    "    #alpha gains should only be positive, 0 to 1 for example\n",
    "    for alpha_g in np.arange(amin, amax, astep):\n",
    "        \n",
    "        #alpha_l will be the inverse of alpha_g\n",
    "        #alpha_l = 1/alpha_g\n",
    "        \n",
    "        #alpha loss should only be positive, 0 to 1 for example\n",
    "        for denom in np.arange(amin, amax, astep):\n",
    "            alpha_l = 1/denom\n",
    "            \n",
    "            #for beta in np.arange(0, 20, 5):\n",
    "            beta = given_beta\n",
    "            alpha_g, alpha_l, beta = np.round(alpha_g, 1), np.round(alpha_l, 3), np.round(beta, 1)\n",
    "            agent = Qagent(alpha_g=alpha_g, alpha_l=alpha_l, beta=beta)\n",
    "            data = agent.play_IGT(ntrials=100, get_output=True)\n",
    "            scores = get_IGT_scores(data)\n",
    "            payoff, sensitivity = scores.iloc[0], scores.iloc[1]\n",
    "            trial_df = pd.DataFrame([[alpha_g, alpha_l, beta, payoff, sensitivity]], columns = df_columns)\n",
    "            df = df.append(trial_df)\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_agent(given_beta):\n",
    "    df = agent_df(given_beta)\n",
    "    plot_heatmaps(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf = agent_df(5, 20, .2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_noncarrier_comparison(df):\n",
    "    \n",
    "    # Creating agent Payoff df\n",
    "    agent_full_df = df\n",
    "    agent_P_df = agent_full_df[\"Payoff\"]\n",
    "    \n",
    "    # Creating Noncarrier Payoff df\n",
    "    full_df = pd.read_csv(\"DRD2_IGT_subset_data.csv\")\n",
    "    P_df = full_df[full_df[\"IGT_score_type\"] == \"P\"]\n",
    "    noncarrier_P_df = P_df[P_df[\"DRD2\"] == 0]\n",
    "    \n",
    "    # Plotting histogram for Noncarrier Payoff scores\n",
    "    plt.subplot(1, 2, 1, constrained_layout=True)\n",
    "    noncarrier_P_hist = noncarrier_P_df[\"IGT_scores\"].hist(bins=50)\n",
    "    plt.axvline(noncarrier_P_df[\"IGT_scores\"].mean(), color='k', linestyle='dashed', linewidth=1)\n",
    "    plt.title(\"Noncarrier Payoff with Mean:\" + str(round(noncarrier_P_df[\"IGT_scores\"].mean(), 3)))\n",
    "    \n",
    "    # Plotting histogram for Agent Payoff scores\n",
    "    plt.subplot(1, 2, 2, constrained_layout=True)\n",
    "    agent_hist = pd.to_numeric(agent_P_df).hist(bins=30)\n",
    "    plt.axvline(pd.to_numeric(agent_P_df).mean(), color='k', linestyle='dashed', linewidth=1)\n",
    "    plt.title(\"Q-Agent Payoff with Mean:\" + str(round(pd.to_numeric(agent_P_df).mean(), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Unknown property constrained_layout",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-cc2508eabe77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent_noncarrier_comparison\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmydf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-66-88b5b1e625ba>\u001b[0m in \u001b[0;36magent_noncarrier_comparison\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Plotting histogram for Noncarrier Payoff scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstrained_layout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mnoncarrier_P_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoncarrier_P_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"IGT_scores\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxvline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoncarrier_P_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"IGT_scores\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinestyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dashed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msubplot\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m     \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0mbyebye\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubplot_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_subplots.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# _axes_class is set in the subplot_class_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axes_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;31m# add a layout box to this, for both the full axis, and the poss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# of the axis.  We need both because the axes may become smaller\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, rect, facecolor, frameon, sharex, sharey, label, xscale, yscale, **kwargs)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, props)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m             ret = [_update_property(self, k, v)\n\u001b[0;32m--> 888\u001b[0;31m                    for k, v in props.items()]\n\u001b[0m\u001b[1;32m    889\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meventson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m             ret = [_update_property(self, k, v)\n\u001b[0;32m--> 888\u001b[0;31m                    for k, v in props.items()]\n\u001b[0m\u001b[1;32m    889\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meventson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36m_update_property\u001b[0;34m(self, k, v)\u001b[0m\n\u001b[1;32m    879\u001b[0m                 \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'set_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown property %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Unknown property constrained_layout"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent_noncarrier_comparison(mydf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_optimal_agent(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_optimal_agent(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_b0 = agent_df(0) #first\n",
    "data_b_2 = agent_df(0.2)\n",
    "data_b_4 = agent_df(0.4)#second\n",
    "data_b_6 = agent_df(0.6)\n",
    "data_b_8 = agent_df(0.8)\n",
    "\n",
    "data_b1 = agent_df(1)\n",
    "data_b2 = agent_df(2)\n",
    "data_b3 = agent_df(3)\n",
    "data_b4 = agent_df(4)\n",
    "\n",
    "data_b5 = agent_df(5)#third\n",
    "data_b10 = agent_df(10)\n",
    "data_b15 = agent_df(15)#fourth\n",
    "data_b30 = agent_df(30)\n",
    "data_b50 = agent_df(50)\n",
    "data_b100 = agent_df(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "magnitude of learning\n",
    "greediness of decision policy\n",
    "assymetry of decision policy\n",
    "\n",
    "(looka t this whole space)\n",
    "\n",
    "3d map --> sample, alpha, ratio, amplitudes, amplitudes of beta, giant map of payoff scores, sensitivty for configurations\n",
    "\n",
    "find baseline parameters, lowest beta, lowest alpha, symmetrical learning rates\n",
    "\n",
    "new repo, and porting this moves over the code, runs this notebook first, \n",
    "\n",
    "pulling from actual decks\n",
    "\n",
    "2D heat map for assymetry of learning and ... greedy, moderate agents etc, and how they very along those dimensions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
